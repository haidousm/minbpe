{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_byte_pair_freqs, get_bytes, merge_until_vocab_size\n",
    "from basic_tokenizer import BasicTokenizer\n",
    "\n",
    "test_str = \"Convert you BasicTokenizer into a RegexTokenizer, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 111, 110, 118, 101, 114, 116, 32, 121, 111, 117, 32, 66, 97, 115, 105, 99, 84, 111, 107, 101, 110, 105, 122, 101, 114, 32, 105, 110, 116, 111, 32, 97, 32, 82, 101, 103, 101, 120, 84, 111, 107, 101, 110, 105, 122, 101, 114, 44, 32, 119, 104, 105, 99, 104, 32, 116, 97, 107, 101, 115, 32, 97, 32, 114, 101, 103, 101, 120, 32, 112, 97, 116, 116, 101, 114, 110, 32, 97, 110, 100, 32, 115, 112, 108, 105, 116, 115, 32, 116, 104, 101, 32, 116, 101, 120, 116, 32, 101, 120, 97, 99, 116, 108, 121, 32, 97, 115, 32, 71, 80, 84, 45, 52, 32, 119, 111, 117, 108, 100, 46, 32, 80, 114, 111, 99, 101, 115, 115, 32, 116, 104, 101, 32, 112, 97, 114, 116, 115, 32, 115, 101, 112, 97, 114, 97, 116, 101, 108, 121, 32, 97, 115, 32, 98, 101, 102, 111, 114, 101, 44, 32, 116, 104, 101, 110, 32, 99, 111, 110, 99, 97, 116, 101, 110, 97, 116, 101, 32, 116, 104, 101, 32, 114, 101, 115, 117, 108, 116, 115, 46, 32, 82, 101, 116, 114, 97, 105, 110, 32, 121, 111, 117, 114, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 97, 110, 100, 32, 99, 111, 109, 112, 97, 114, 101, 32, 116, 104, 101, 32, 114, 101, 115, 117, 108, 116, 115, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 89, 111, 117, 32, 115, 104, 111, 117, 108, 100, 32, 115, 101, 101, 32, 116, 104, 97, 116, 32, 121, 111, 117, 32, 119, 105, 108, 108, 32, 110, 111, 119, 32, 104, 97, 118, 101, 32, 110, 111, 32, 116, 111, 107, 101, 110, 115, 32, 116, 104, 97, 116, 32, 103, 111, 32, 97, 99, 114, 111, 115, 115, 32, 99, 97, 116, 101, 103, 111, 114, 105, 101, 115, 32, 40, 110, 117, 109, 98, 101, 114, 115, 44, 32, 108, 101, 116, 116, 101, 114, 115, 44, 32, 112, 117, 110, 99, 116, 117, 97, 116, 105, 111, 110, 44, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 111, 110, 101, 32, 119, 104, 105, 116, 101, 115, 112, 97, 99, 101, 41, 46, 32, 85, 115, 101, 32, 116, 104, 101, 32, 71, 80, 84, 45, 52, 32, 112, 97, 116, 116, 101, 114, 110, 58]\n"
     ]
    }
   ],
   "source": [
    "bytes = get_bytes(test_str)\n",
    "print(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freqs = get_byte_pair_freqs(bytes)\n",
    "# freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n",
      "293\n",
      "{(32, 116): 256, (101, 114): 257, (32, 97): 258, (97, 116): 259, (256, 104): 260, (114, 101): 261, (111, 117): 262, (101, 110): 263, (115, 32): 264, (101, 32): 265, (111, 110): 266, (111, 107): 267, (267, 263): 268, (101, 120): 269, (44, 32): 270, (116, 257): 271, (46, 32): 272, (260, 265): 273, (112, 97): 274, (32, 121): 275}\n"
     ]
    }
   ],
   "source": [
    "pre_merge_len = len(bytes)\n",
    "merged_bytes, merges = merge_until_vocab_size(bytes, 276)\n",
    "post_merge_len = len(merged_bytes)\n",
    "\n",
    "print(pre_merge_len)\n",
    "print(post_merge_len)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99]\n",
      "[256, 97, 98, 100, 256, 97, 98, 97, 99]\n",
      "[257, 98, 100, 257, 98, 97, 99]\n",
      "[258, 100, 258, 97, 99]\n",
      "True\n",
      "[258, 100, 258, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "basic_tokenizer = BasicTokenizer()\n",
    "test_str = \"aaabdaaabac\"\n",
    "basic_tokenizer.train(test_str, 256 + 3)\n",
    "ids = basic_tokenizer.encode(test_str)\n",
    "print(ids == [258, 100, 258, 97, 99]) # ground truth\n",
    "print(basic_tokenizer.decode(ids) == test_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
